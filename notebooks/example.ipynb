{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../python\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Dataloader\n",
    "from models import RandomExampleModel\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data_loader = Dataloader(\"../data/dataset_4\") # Specify path to the dataset you want to load\n",
    "X, Y = data_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number 0\n",
      "Test number 1\n",
      "Test number 2\n",
      "Test number 3\n",
      "Test number 4\n",
      "Test number 5\n",
      "Test number 6\n",
      "Test number 7\n",
      "Test number 8\n",
      "Test number 9\n"
     ]
    }
   ],
   "source": [
    "X_Y = np.concatenate((X, Y), axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def compute_distibution_score(X_train,X_test):\n",
    "    mean_train = np.mean(X_train, axis=0)\n",
    "    mean_test = np.mean(X_test, axis=0)\n",
    "    std_train = np.std(X_train, axis=0)\n",
    "    std_test = np.std(X_test, axis=0)\n",
    "    return np.sum(np.abs(mean_train - mean_test)*np.abs(std_train - std_test))\n",
    "\n",
    "def split_train_test_keeping_distribution(X_Y, test_size=0.7, nb_tests=10):\n",
    "    score = np.inf\n",
    "    for i in range(nb_tests):\n",
    "        print(\"Test number {}\".format(i))\n",
    "        X_tr, X_te = train_test_split(X_Y, test_size=test_size, random_state=i)\n",
    "        new_score = compute_distibution_score(X_tr, X_te)\n",
    "        if new_score < score:\n",
    "            X_train, X_test = X_tr, X_te\n",
    "            score = new_score\n",
    "    return X_train, X_test, score\n",
    "\n",
    "X_Y_train, X_Y_test,score = split_train_test_keeping_distribution(X_Y, test_size=0.7, nb_tests=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48150037, 0.46663161, 0.54626564, 0.48732472, 0.47276505,\n",
       "       0.47782507, 0.55242803, 0.47955519])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.49158047, 0.4676945 , 0.5474553 , 0.47755223, 0.48053892,\n",
       "       0.47894819, 0.55045437, 0.4784844 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.29466395, 0.29794311, 0.26250953, 0.29970479, 0.29295507,\n",
       "       0.30931778, 0.2612397 , 0.29965856])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.28798069, 0.29828799, 0.26093057, 0.29627329, 0.28893297,\n",
       "       0.29948195, 0.26462511, 0.30433777])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.mean(X_Y_train, axis=0))\n",
    "display(np.mean(X_Y_test, axis=0))\n",
    "display(np.std(X_Y_train, axis=0))\n",
    "display(np.std(X_Y_test, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00015715347907758543"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train an example model\n",
    "model = RandomExampleModel() # Instantiation of the model with hyperparameters, if needed\n",
    "model.fit(X, Y) # Training of the model, using preference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see how our model reconsiders our preferences\n",
    "# Our data preferences were X[i] >> Y[i] for all i, let's see if our model agrees\n",
    "print((model.predict_utility(X) - model.predict_utility(Y))[:5]) # Preferred item has a higher utility\n",
    "\n",
    "# predict_utility returns utility of samples for each cluster, returning shape (n_samples, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can directly use the predict_preference method:\n",
    "print(model.predict_preference(X, Y)[:5]) # For each cluster we see if X is preferred to Y (0) or the contrary (1)\n",
    "# predict_preference returns preference of samples for each cluster, returning shape (n_samples, n_clusters)\n",
    "# 0 means that the cluster prefers first argument (here X[i]), 1 means that the cluster prefers second argument (here Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider that our model explains our data, if for at least one cluster, X[i] >> Y[i]\n",
    "pairs_explained = np.sum(model.predict_preference(X, Y)[:5] == 0, axis=1) # For each sample check if each clusters prefers X[i] (then is True)\n",
    "pairs_explained = pairs_explained > 0 # If at least one cluster prefers X[i], then X[i] >> Y[i] is explained\n",
    "print(\"Percentage of explained preferences for 5 first samples:\", np.sum(pairs_explained) / 5) # Get % or explained pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we call directly the right metric:\n",
    "pairs_explained = metrics.PairsExplained()\n",
    "print(\"Percentage of explained preferences for all samples:\", pairs_explained.from_model(model, X, Y))\n",
    "# Or other possibility:\n",
    "print(\"Percentage of explained preferences for all samples:\", pairs_explained(model.predict_utility(X), model.predict_utility(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can also see how our model clusters our data\n",
    "print(model.predict_cluster(X, Y)[:5]) # Returns for each sample which cluster is the most likely to have generated the preference.\n",
    "# The most likely cluster is the one with the highest utility difference U(X[i])-U(Y[i]) for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth are also provided:\n",
    "Z = data_loader.get_ground_truth_labels()\n",
    "# We can see how it compares to our model's predictions:\n",
    "print(Z[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a metric to calculate how well the model has regrouped the preferences pairs compared to ground truth:\n",
    "cluster_intersection = metrics.ClusterIntersection()\n",
    "\n",
    "print(\"Cluster intersection for 5 first samples:\", cluster_intersection(model.predict_cluster(X, Y)[:5], Z[:5]))\n",
    "\n",
    "# Then for all data:\n",
    "print(\"Cluster intersection for all samples:\", cluster_intersection(model.predict_cluster(X, Y), Z))\n",
    "print(\"Cluster intersection for all samples:\", cluster_intersection.from_model(model, X, Y, Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a proposition of a useful plot to see how the model clusters the data:\n",
    "# We look at how each cluster evaluates U(x) - U(y) and color with ground truth clustering\n",
    "# Note that x >>_1 y means that x is preferred to y for cluster 1\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"U1(x) - U1(y)\")\n",
    "plt.ylabel(\"U2(x) - U2(y)\")\n",
    "\n",
    "plt.gca().add_patch(Rectangle((0,-1.),1., 2.,\n",
    "                    edgecolor='red',\n",
    "                    facecolor='none',\n",
    "                    lw=0, \n",
    "                    hatch='/'))\n",
    "plt.gca().add_patch(Rectangle((-1.,0), 2., 1.,\n",
    "                    edgecolor='green',\n",
    "                    facecolor='none',\n",
    "                    lw=0, \n",
    "                    hatch='\\\\'))\n",
    "plt.gca().add_patch(Rectangle((-1.,-1.),1., 1.,\n",
    "                    edgecolor='blue',\n",
    "                    facecolor='none',\n",
    "                    hatch='/',\n",
    "                    lw=0))\n",
    "plt.scatter(model.predict_utility(X)[:, 0]-model.predict_utility(Y)[:, 0], \n",
    "            model.predict_utility(X)[:, 1]-model.predict_utility(Y)[:, 1], c=Z)\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color=\"red\", lw=4),\n",
    "                Line2D([0], [0], color=\"green\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4)]\n",
    "\n",
    "plt.legend(custom_lines, ['x >>_1 y', \"x >>_2 y \", 'No preference of x over y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random model cannot explain a large part of our preference data. Your first assigment will be to write a Mixed-Interger Programming algorithm that will be able to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should work when you have completed the TwoClustersMIP model\n",
    "from models import TwoClustersMIP\n",
    "\n",
    "parameters = {\"n_pieces\": 5,} # Can be completed\n",
    "model = TwoClustersMIP(**parameters)\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Uncomment once your model is working\n",
    "# print(\"Percentage of explained preferences on train data:\", pairs_explained.from_model(model, X, Y)) # You should get 1.0 with the right MIP\n",
    "# print(\"Percentage of preferences well regrouped into clusters:\", cluster_intersection.from_model(model, X, Y, Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example here are the results obtained with:\n",
    "\"\"\" # Uncomment the code to check your results\n",
    "plt.figure()\n",
    "plt.scatter(model.predict_utility(X)[:, 0]-model.predict_utility(Y)[:, 0], \n",
    "            model.predict_utility(X)[:, 1]-model.predict_utility(Y)[:, 1], c=Z)\n",
    "plt.xlabel(\"U1(x) - U1(y)\")\n",
    "plt.ylabel(\"U2(x) - U2(y)\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Here is how it should look like\n",
    "from IPython.display import Image\n",
    "Image(\"../images/MIP_results_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second assignement will be to find another model that will work well with larger data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
